{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gMvnVozn1G56",
        "outputId": "e5370145-96e9-48e5-efa0-0b5428437270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1KLn3NLLv2rng2vV_8Ys3Yn53iahv9shd\n",
            "From (redirected): https://drive.google.com/uc?id=1KLn3NLLv2rng2vV_8Ys3Yn53iahv9shd&confirm=t&uuid=2945bca2-e802-4894-8e6b-e83293c73ef3\n",
            "To: /content/Suicide_Detection.csv\n",
            "100% 167M/167M [00:04<00:00, 37.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1KLn3NLLv2rng2vV_8Ys3Yn53iahv9shd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Load Dataset\n",
        "dataset_path = \"Suicide_Detection.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "texts = df['text'].astype(str).values\n",
        "labels = df['class'].values\n",
        "\n",
        "# Encode labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Tokenize texts\n",
        "max_vocab_size = 20000\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Model Building\n",
        "embedding_dim = 128\n",
        "\n",
        "input_layer = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(input_layer)\n",
        "# Use unroll=True for LSTM compatibility with TFLite\n",
        "lstm_layer = LSTM(64, return_sequences=True, unroll=True)(embedding_layer)\n",
        "global_pooling = GlobalMaxPooling1D()(lstm_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(global_pooling)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# 4. Compile Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 5. Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# 6. Save Model\n",
        "model.save(\"suicide_detection_model.keras\")\n",
        "print(\"Model saved as suicide_detection_model.keras\")\n",
        "\n",
        "# 7. Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Apply default optimizations\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open(\"suicide_detection_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model converted and saved as suicide_detection_model.tflite\")"
      ],
      "metadata": {
        "id": "TqrUcVvV1Mc6",
        "outputId": "15b2dc24-6109-4901-c328-d90a1eec2e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8862 - loss: 0.2811Epoch 2/5\n",
            "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 9ms/step - accuracy: 0.9449 - loss: 0.1504 - val_accuracy: 0.9376 - val_loss: 0.1627\n",
            "Epoch 3/5\n",
            "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9565 - loss: 0.1220 - val_accuracy: 0.9389 - val_loss: 0.1646\n",
            "Epoch 4/5\n",
            "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9633 - loss: 0.1028 - val_accuracy: 0.9387 - val_loss: 0.1723\n",
            "Epoch 5/5\n",
            "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9714 - loss: 0.0821 - val_accuracy: 0.9369 - val_loss: 0.1821\n",
            "Model saved as suicide_detection_model.keras\n",
            "Saved artifact at '/tmp/tmp0fvdajod'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134955479477488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134955437882736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134955437875872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134955437872352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134955316743152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134955316741744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model converted and saved as suicide_detection_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence_tflite(interpreter, tokenizer, sentence, max_sequence_length):\n",
        "    \"\"\"\n",
        "    Fungsi untuk memprediksi label dari kalimat input menggunakan model TFLite.\n",
        "\n",
        "    Args:\n",
        "        interpreter: TensorFlow Lite interpreter yang sudah dimuat.\n",
        "        tokenizer: Tokenizer yang digunakan saat pelatihan.\n",
        "        sentence: Kalimat input (string) yang ingin diprediksi.\n",
        "        max_sequence_length: Panjang maksimum urutan (sama seperti saat melatih model).\n",
        "\n",
        "    Returns:\n",
        "        Prediksi label ('suicide' atau 'non suicide') untuk kalimat input.\n",
        "    \"\"\"\n",
        "    # Preprocess input sentence\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Set input tensor\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    interpreter.allocate_tensors()\n",
        "    interpreter.set_tensor(input_details[0]['index'], padded_sequence.astype(np.float32))\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the prediction result\n",
        "    prediction = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
        "\n",
        "    # Interpret prediction\n",
        "    label = 'suicide' if prediction >= 0.5 else 'non suicide'\n",
        "    return label\n",
        "\n",
        "# 9. Load TFLite Model for Inference\n",
        "interpreter = tf.lite.Interpreter(model_path=\"suicide_detection_model.tflite\")"
      ],
      "metadata": {
        "id": "GVtYKGZP1Q1y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"life is wonderful, i like being alive\"\n",
        "predicted_label = predict_sentence_tflite(interpreter, tokenizer, sentence, max_sequence_length)\n",
        "print(f\"Kalimat: \\\"{sentence}\\\"\")\n",
        "print(f\"Prediksi: {predicted_label}\")"
      ],
      "metadata": {
        "id": "rszlWHOw1Tul",
        "outputId": "8e755580-aff2-49e4-a1d3-393edd26dd68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat: \"life is wonderful, i like being alive\"\n",
            "Prediksi: non suicide\n"
          ]
        }
      ]
    }
  ]
}